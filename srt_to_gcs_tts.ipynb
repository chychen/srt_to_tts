{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pydub pysrt google-cloud-texttospeech==1.0.1 \n",
    "# (>2.0.0 failed)\n",
    "# https://cloud.google.com/text-to-speech/docs/voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"../googlecloud.json\"\n",
    "from google.cloud import texttospeech\n",
    "from pydub import AudioSegment\n",
    "import io\n",
    "import pysrt\n",
    "\n",
    "VEDIO_IDX = 5 # PLEASE MODIFY HERE\n",
    "LANGUAGES =  [\n",
    "    'kr', \n",
    "    'vn', \n",
    "    'th', \n",
    "    'en', \n",
    "    'zh',\n",
    "    'jp'\n",
    "]\n",
    "\n",
    "# Build the voice request, select the language code (\"en-US\") and the ssml\n",
    "# voice gender (\"neutral\")\n",
    "VOICES = {}\n",
    "VOICES['kr'] = texttospeech.types.VoiceSelectionParams(\n",
    "    language_code='ko-KR', name='ko-KR-Wavenet-A',\n",
    "    ssml_gender=texttospeech.enums.SsmlVoiceGender.NEUTRAL)\n",
    "VOICES['vn'] = texttospeech.types.VoiceSelectionParams(\n",
    "    language_code='vi-VN', name='vi-VN-Wavenet-A',\n",
    "    ssml_gender=texttospeech.enums.SsmlVoiceGender.NEUTRAL)\n",
    "VOICES['th'] = texttospeech.types.VoiceSelectionParams(\n",
    "    language_code='th-TH', name='th-TH-Standard-A',\n",
    "    ssml_gender=texttospeech.enums.SsmlVoiceGender.NEUTRAL)\n",
    "VOICES['en'] = texttospeech.types.VoiceSelectionParams(\n",
    "    language_code='en-US', name='en-US-Wavenet-F',\n",
    "    ssml_gender=texttospeech.enums.SsmlVoiceGender.NEUTRAL)\n",
    "VOICES['zh'] = texttospeech.types.VoiceSelectionParams(\n",
    "    language_code='cmn-TW', name='cmn-TW-Wavenet-A',\n",
    "    ssml_gender=texttospeech.enums.SsmlVoiceGender.NEUTRAL)\n",
    "VOICES['jp'] = texttospeech.types.VoiceSelectionParams(\n",
    "    language_code='ja-JP', name='ja-JP-Wavenet-B',\n",
    "    ssml_gender=texttospeech.enums.SsmlVoiceGender.NEUTRAL)\n",
    "\n",
    "# Instantiates a client\n",
    "client = texttospeech.TextToSpeechClient()\n",
    "# Select the type of audio file you want returned\n",
    "audio_config = texttospeech.types.AudioConfig(\n",
    "    audio_encoding=texttospeech.enums.AudioEncoding.MP3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_ms(sub_time):\n",
    "    return (sub_time.hours * sub_time.HOURS_RATIO + \n",
    "            sub_time.minutes * sub_time.MINUTES_RATIO + \n",
    "            sub_time.seconds * sub_time.SECONDS_RATIO + \n",
    "            sub_time.milliseconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for LANGUAGE in LANGUAGES:\n",
    "    parser = pysrt.open(f'{LANGUAGE}_{VEDIO_IDX}.srt')\n",
    "    voice = VOICES[LANGUAGE]\n",
    "    \n",
    "    all_ = AudioSegment.empty()\n",
    "    # first silence\n",
    "    d_silent = total_ms(parser[0].start)\n",
    "    silence = AudioSegment.silent(duration=d_silent)\n",
    "    all_ = all_ + silence\n",
    "    for i, (sub, next_sub) in enumerate(zip(parser[:-1],parser[1:])):\n",
    "        if i%50==0:\n",
    "            print(f'{i}/{len(parser)}')\n",
    "\n",
    "        # Set the text input to be synthesized\n",
    "        synthesis_input = texttospeech.types.SynthesisInput(text=sub.text)\n",
    "        # Perform the text-to-speech request on the text input with the selected\n",
    "        # voice parameters and audio file type\n",
    "        response = client.synthesize_speech(synthesis_input, voice, audio_config)    \n",
    "\n",
    "        audio_sub = io.BytesIO(response.audio_content)\n",
    "        audio_sub = AudioSegment.from_file(audio_sub)\n",
    "        all_ = all_ + audio_sub\n",
    "        d_silent = total_ms(next_sub.start) - all_.duration_seconds*1000\n",
    "        silence = AudioSegment.silent(duration=d_silent)\n",
    "        all_ = all_ + silence\n",
    "    # last sub\n",
    "    sub = parser[-1]\n",
    "    synthesis_input = texttospeech.types.SynthesisInput(text=sub.text)\n",
    "    response = client.synthesize_speech(synthesis_input, voice, audio_config)    \n",
    "    audio_sub = io.BytesIO(response.audio_content)\n",
    "    audio_sub = AudioSegment.from_file(audio_sub)\n",
    "    all_ = all_ + audio_sub\n",
    "    all_.export(f'../audios/{LANGUAGE}_{VEDIO_IDX}.mp3')\n",
    "    print(all_.duration_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
